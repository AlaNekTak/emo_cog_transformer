Singularity> /home1/nekouvag/projects/emo_cog/myenv/bin/python /home1/nekouvag/projects/emo_cog/Main.py
07-Jul-24 13:56:40 - INFO - CONFIG device: cuda

07-Jul-24 13:56:40 - INFO - CONFIG model_name: roberta-base

07-Jul-24 13:56:40 - INFO - CONFIG tokenizer: RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
        0: AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        1: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        3: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
        50264: AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),
}

07-Jul-24 13:56:40 - INFO - CONFIG use_input_embeddings: False

07-Jul-24 13:56:40 - INFO - CONFIG load_from_checkpoint: False

07-Jul-24 13:56:40 - INFO - CONFIG checkpoint_dir: /home1/nekouvag/projects/emo_cog/lightning_logs/2024_06_30__13_02_59/checkpoints/model-epoch-00-val_loss-2.65.ckpt

07-Jul-24 13:56:40 - INFO - CONFIG inspect_data: False

07-Jul-24 13:56:40 - INFO - CONFIG quick_test: False

07-Jul-24 13:56:40 - INFO - CONFIG mode: both

07-Jul-24 13:56:40 - INFO - CONFIG expert_mode: mixed

07-Jul-24 13:56:40 - INFO - CONFIG random_state: 42

07-Jul-24 13:56:40 - INFO - CONFIG train_batch_size: 32

07-Jul-24 13:56:40 - INFO - CONFIG val_batch_size: 32

07-Jul-24 13:56:40 - INFO - CONFIG train_val_split: 0.1

07-Jul-24 13:56:40 - INFO - CONFIG hparam_trials: 1

07-Jul-24 13:56:40 - INFO - CONFIG optimizer_lr: [1e-05]

07-Jul-24 13:56:40 - INFO - CONFIG optimizer_batch_size: [32]

07-Jul-24 13:56:40 - INFO - CONFIG max_length: 512

07-Jul-24 13:56:40 - INFO - CONFIG lr: 1e-05

07-Jul-24 13:56:40 - INFO - CONFIG warmup: 0.1

07-Jul-24 13:56:40 - INFO - CONFIG weight_decay: 0.001

07-Jul-24 13:56:40 - INFO - CONFIG epochs: 18

07-Jul-24 13:56:40 - INFO - CONFIG dropout_rate: 0.2

07-Jul-24 13:56:40 - INFO - CONFIG log_path: lightning_logs

07-Jul-24 13:56:40 - INFO - CONFIG train_path: data/enVent_new_Data_train.csv

07-Jul-24 13:56:40 - INFO - CONFIG val_path: data/enVent_new_Data_val.csv

07-Jul-24 13:56:40 - INFO - CONFIG test_path: data/enVent_new_Data_test.csv

07-Jul-24 13:56:40 - INFO - CONFIG data_encoding: ISO-8859-1

07-Jul-24 13:56:40 - INFO - CONFIG gate_mechanism: soft

07-Jul-24 13:56:40 - INFO - CONFIG train_data:      round_number  ...          original_demographics
0         round-5  ...     COPIED FROM: text id 51143
1         round-3  ...                       ORIGINAL
2         round-4  ...                       ORIGINAL
3         round-6  ...      COPIED FROM: text id 6832
4         round-4  ...      COPIED FROM: text id 4147
...           ...  ...                            ...
4855      round-6  ...       COPIED FROM: text id 677
4856      round-5  ...  AVERAGED: text id 5210, 52656
4857      round-5  ...  AVERAGED: text id 5959, 52707
4858      round-6  ...      COPIED FROM: text id 6620
4859      round-6  ...      COPIED FROM: text id 6832

[4860 rows x 61 columns]

07-Jul-24 13:56:40 - INFO - CONFIG val_data:     round_number  ...                original_demographics
0        round-2  ...            COPIED FROM: text id 2102
1        round-4  ...   COPIED FROM REJECTED: text id 4192
2        round-5  ...           COPIED FROM: text id 52349
3        round-5  ...                             ORIGINAL
4        round-3  ...                             ORIGINAL
..           ...  ...                                  ...
535      round-5  ...  COPIED FROM REJECTED: text id 51703
536      round-4  ...           COPIED FROM: text id 41834
537      round-4  ...   COPIED FROM REJECTED: text id 4417
538      round-6  ...   COPIED FROM REJECTED: text id 6727
539      round-6  ...           AVERAGED: text id 61, 6313

[540 rows x 61 columns]

07-Jul-24 13:56:40 - INFO - CONFIG text: hidden_emo_text

07-Jul-24 13:56:40 - INFO - CONFIG peft_config_roberta: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=4, target_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})

07-Jul-24 13:56:40 - INFO - CONFIG emotion_or_appraisal: both

07-Jul-24 13:56:40 - INFO - CONFIG emo_dict: {'anger': 0, 'boredom': 1, 'disgust': 2, 'fear': 3, 'guilt': 4, 'joy': 5, 'no-emotion': 6, 'pride': 7, 'relief': 8, 'sadness': 9, 'shame': 10, 'surprise': 11, 'trust': 12}

07-Jul-24 13:56:40 - INFO - CONFIG class_num: 13

07-Jul-24 13:56:40 - INFO - CONFIG emo_attributes: ['emotion']

07-Jul-24 13:56:40 - INFO - CONFIG n_emo_attributes: 1

07-Jul-24 13:56:40 - INFO - CONFIG attributes: ['predict_event', 'pleasantness', 'attention', 'other_responsblt', 'chance_control', 'social_norms']

07-Jul-24 13:56:40 - INFO - CONFIG n_attributes: 6

07-Jul-24 13:56:40 - INFO - CONFIG train_size: 4860

07-Jul-24 13:56:40 - INFO - Current Working Directory:/home1/nekouvag/projects/emo_cog
07-Jul-24 13:56:40 - INFO - CUDA_VISIBLE_DEVICES: 0
07-Jul-24 13:56:40 - INFO - Using device: cuda
[rank: 0] Seed set to 42
07-Jul-24 13:56:40 - INFO - TOKENIZERS_PARALLELISM set to false
[I 2024-07-07 13:56:40,552] A new study created in memory with name: MyOptimizationStudy
07-Jul-24 13:56:40 - INFO - Trial 0, lr: 1e-05, train batch size: 32
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home1/nekouvag/projects/emo_cog/myenv/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home1/nekouvag/projects/emo_cog/Main.py ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home1/nekouvag/projects/emo_cog/myenv/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home1/nekouvag/projects/emo_cog/Main.py ...
You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name                  | Type             | Params
-----------------------------------------------------------
0 | pretrained_model      | RobertaModel     | 124 M 
1 | appraisal_hidden      | ModuleList       | 3.5 M 
2 | appraisal_classifiers | ModuleList       | 4.6 K 
3 | dynamic_gate          | DynamicSoftGate  | 4.6 K 
4 | emotion_hidden        | Linear           | 3.5 M 
5 | emotion_classifier    | Linear           | 10.0 K
6 | appraisal_loss_func   | MSELoss          | 0     
7 | emotion_loss_func     | CrossEntropyLoss | 0     
8 | dropout               | Dropout          | 0     
-----------------------------------------------------------
131 M     Trainable params
0         Non-trainable params
131 M     Total params
526.992   Total estimated model params size (MB)
Sanity Checking DataLoader 0: 100%|â| 2/2 [00:07-Jul-24 13:56:54 - INFO - Validation ended. Epoch 0  val_loss: 6.683197975158691

07-Jul-24 13:56:55 - INFO -                   

Training is starting...
--------------------------
Epoch 0: 100%|âââââââââââââââââ| 152/152 [01:42<00:00,  1.48it/s, v_num=6_4007-Jul-24 13:58:42 - INFO - Validation ended. Epoch 0  val_loss: 5.107832431793213

Epoch 0: 100%|â| 152/152 [01:46<00:00,  1.42it/s, v_num=6_40, val_loss=5.110,predict_event weight:  0.15943713                                            
predict_event loss:  0.7555178
pleasantness weight:  0.16358633
pleasantness loss:  0.6709358
attention weight:  0.18428776
attention loss:  0.8190354
other_responsblt weight:  0.20783798
other_responsblt loss:  0.7934709
chance_control weight:  0.16039446
chance_control loss:  0.6728915
social_norms weight:  0.12445622
social_norms loss:  0.5488722
emotion loss:  0.988704
Epoch 1: 100%|âââââââââââ| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=5.110, emotion_loss=0.989, train_loss=5.25007-Jul-24 14:00:45 - INFO - Validation ended. Epoch 1  val_loss: 2.2330994606018066âââââââââââ| 17/17 [00:03<00:00,  4.43it/s]

Epoch 1: 100%|âââââââââââ| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=2.230, emotion_loss=0.984, train_loss=3.940]predict_event weight:  0.18616503                                                                                                                           
predict_event loss:  0.5458393
pleasantness weight:  0.16397925
pleasantness loss:  0.4443555
attention weight:  0.17282216
attention loss:  0.5538365
other_responsblt weight:  0.17735572
other_responsblt loss:  0.5542524
chance_control weight:  0.16529053
chance_control loss:  0.45349926
social_norms weight:  0.13438737
social_norms loss:  0.40056407
emotion loss:  0.9842416
Epoch 2: 100%|âââââââââââ| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=2.230, emotion_loss=0.984, train_loss=3.94007-Jul-24 14:02:47 - INFO - Validation ended. Epoch 2  val_loss: 2.000669240951538ââââââââââââ| 17/17 [00:03<00:00,  4.44it/s]

Epoch 2: 100%|âââââââââââ| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=2.000, emotion_loss=0.975, train_loss=2.590]predict_event weight:  0.23602365                                                                                                                           
predict_event loss:  0.25186124
pleasantness weight:  0.13949955
pleasantness loss:  0.19578885
attention weight:  0.14392287
attention loss:  0.28486034
other_responsblt weight:  0.12304272
other_responsblt loss:  0.26063535
chance_control weight:  0.20726416
chance_control loss:  0.33058405
social_norms weight:  0.15024714
social_norms loss:  0.2899104
emotion loss:  0.97468233
Epoch 3:  95%|â| 144/152 [01:38<00:05,  1.46it/s, v_num=6_40, val_loss=                                                                       Epoch 3: 100%|â| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss07-Jul-24 14:04:50 - INFO - Validation ended. Epoch 3  val_loss: 1.8642748594284058

Epoch 3: 100%|â| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=predict_event weight:  0.24862722                                      
predict_event loss:  0.23468837
pleasantness weight:  0.12745033
pleasantness loss:  0.15474042
attention weight:  0.14080754
attention loss:  0.27912804
other_responsblt weight:  0.13077414
other_responsblt loss:  0.23533928
chance_control weight:  0.20873149
chance_control loss:  0.30445343
social_norms weight:  0.14360924
social_norms loss:  0.2647513
emotion loss:  0.9563988
Epoch 4: 100%|â| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss07-Jul-24 14:06:53 - INFO - Validation ended. Epoch 4  val_loss: 1.777079463005066

Epoch 4: 100%|â| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=predict_event weight:  0.2929081                                       
predict_event loss:  0.23184852
pleasantness weight:  0.123375334
pleasantness loss:  0.1527864
attention weight:  0.14757375
attention loss:  0.27093834
other_responsblt weight:  0.12952095
other_responsblt loss:  0.22144023
chance_control weight:  0.18376713
chance_control loss:  0.2966696
social_norms weight:  0.12285474
social_norms loss:  0.2502186
emotion loss:  0.91698754
Epoch 5: 100%|â| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss07-Jul-24 14:08:56 - INFO - Validation ended. Epoch 5  val_loss: 1.7216873168945312

Epoch 5: 100%|â| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=predict_event weight:  0.38778636                                      
predict_event loss:  0.22102952
pleasantness weight:  0.12603435
pleasantness loss:  0.15312599
attention weight:  0.17793168
attention loss:  0.25520298
other_responsblt weight:  0.09176994
other_responsblt loss:  0.21389447
chance_control weight:  0.13754253
chance_control loss:  0.28067833
social_norms weight:  0.07893535
social_norms loss:  0.23519857
emotion loss:  0.8370465
Epoch 6:  26%|ââ      | 39/152 [00:26<01:17,  1.46it/s, v_numEpoch 6:  26%|â| 40/152 [00:27<01:16,  1.46it/s, v_num=6_40, Epoch 6: 100%|â| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.720, emotion_loss=0.837, train_loss=2.20007-Jul-24 14:10:58 - INFO - Validation ended. Epoch 6  val_loss: 1.5523136854171753â| 17/17 [00:03<00:00,  4.43it/s]

Epoch 6: 100%|â| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.550, emotion_loss=0.742, train_loss=2.090]predict_event weight:  0.46973607                                                                                                                                                                               
predict_event loss:  0.22402859
pleasantness weight:  0.13295045
pleasantness loss:  0.14522724
attention weight:  0.205354
attention loss:  0.2630679
other_responsblt weight:  0.061610747
other_responsblt loss:  0.21313249
chance_control weight:  0.08682271
chance_control loss:  0.27690122
social_norms weight:  0.043525998
social_norms loss:  0.23014218
emotion loss:  0.74164844
Epoch 7: 100%|â| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.550, emotion_loss=0.742, train_loss=2.09007-Jul-24 14:13:01 - INFO - Validation ended. Epoch 7  val_loss: 1.5036360025405884â| 17/17 [00:03<00:00,  4.43it/s]

Epoch 7: 100%|â| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.500, emotion_loss=0.639, train_loss=1.960]predict_event weight:  0.50164086                                                                                                                                                                               
predict_event loss:  0.21338777
pleasantness weight:  0.13730094
pleasantness loss:  0.14495859
attention weight:  0.23966868
attention loss:  0.2578177
other_responsblt weight:  0.040521383
other_responsblt loss:  0.2053234
chance_control weight:  0.054655336
chance_control loss:  0.27747977
social_norms weight:  0.026212705
social_norms loss:  0.2226664
emotion loss:  0.6394362
Epoch 8: 100%|â| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.500, emotion_loss=0.639, train_loss=1.96007-Jul-24 14:15:04 - INFO - Validation ended. Epoch 8  val_loss: 1.3863394260406494â| 17/17 [00:03<00:00,  4.43it/s]

Epoch 8: 100%|â| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.390, emotion_loss=0.537, train_loss=1.830]predict_event weight:  0.51389146                                                                                                                                                                               
predict_event loss:  0.21436931
pleasantness weight:  0.13298103
pleasantness loss:  0.13524228
attention weight:  0.27564192
attention loss:  0.2660096
other_responsblt weight:  0.026532602
other_responsblt loss:  0.20262273
chance_control weight:  0.034143165
chance_control loss:  0.26507756
social_norms weight:  0.016809672
social_norms loss:  0.21056467
emotion loss:  0.5372435
Epoch 9: 100%|â| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.390, emotion_loss=0.537, train_loss=1.83007-Jul-24 14:17:07 - INFO - Validation ended. Epoch 9  val_loss: 1.4053202867507935â| 17/17 [00:03<00:00,  4.43it/s]

Epoch 9: 100%|â| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.410, emotion_loss=0.450, train_loss=1.700]predict_event weight:  0.52192485                                                                                                                                                                               
predict_event loss:  0.20545565
pleasantness weight:  0.12653095
pleasantness loss:  0.13959414
attention weight:  0.2943409
attention loss:  0.25398833
other_responsblt weight:  0.019870924
other_responsblt loss:  0.19936582
chance_control weight:  0.024356654
chance_control loss:  0.2517359
social_norms weight:  0.012975595
social_norms loss:  0.19689576
emotion loss:  0.44955006
Epoch 10: 100%|| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.410, emotion_loss=0.450, train_loss=1.70007-Jul-24 14:19:09 - INFO - Validation ended. Epoch 10  val_loss: 1.4113768339157104| 17/17 [00:03<00:00,  4.43it/s]

Epoch 10: 100%|| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.410, emotion_loss=0.374, train_loss=1.560]predict_event weight:  0.5244301                                                                                                                                                                                
predict_event loss:  0.19926848
pleasantness weight:  0.12263735
pleasantness loss:  0.13170674
attention weight:  0.30530474
attention loss:  0.23980087
other_responsblt weight:  0.016551545
other_responsblt loss:  0.20145215
chance_control weight:  0.020010429
chance_control loss:  0.2266478
social_norms weight:  0.011065729
social_norms loss:  0.18539499
emotion loss:  0.37396517
Epoch 11: 100%|| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.410, emotion_loss=0.374, train_loss=1.56007-Jul-24 14:21:13 - INFO - Validation ended. Epoch 11  val_loss: 1.3971014022827148| 17/17 [00:03<00:00,  4.43it/s]

Epoch 11: 100%|| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.400, emotion_loss=0.303, train_loss=1.450]predict_event weight:  0.5239459                                                                                                                                                                                
predict_event loss:  0.1890138
pleasantness weight:  0.12157236
pleasantness loss:  0.14229351
attention weight:  0.31290236
attention loss:  0.23999862
other_responsblt weight:  0.013859523
other_responsblt loss:  0.18517056
chance_control weight:  0.017785938
chance_control loss:  0.21297519
social_norms weight:  0.0099339485
social_norms loss:  0.1788668
emotion loss:  0.30252612
Epoch 12: 100%|| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.400, emotion_loss=0.303, train_loss=1.45007-Jul-24 14:23:15 - INFO - Validation ended. Epoch 12  val_loss: 1.4551161527633667| 17/17 [00:03<00:00,  4.43it/s]

Epoch 12: 100%|| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.460, emotion_loss=0.233, train_loss=1.320]predict_event weight:  0.52418005                                                                                                                                                                               
predict_event loss:  0.18161425
pleasantness weight:  0.121626675
pleasantness loss:  0.12614663
attention weight:  0.31694397
attention loss:  0.22647043
other_responsblt weight:  0.012112253
other_responsblt loss:  0.18787052
chance_control weight:  0.01604664
chance_control loss:  0.20323214
social_norms weight:  0.009090404
social_norms loss:  0.16418266
emotion loss:  0.23287536
Epoch 13: 100%|| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.460, emotion_loss=0.233, train_loss=1.32007-Jul-24 14:25:18 - INFO - Validation ended. Epoch 13  val_loss: 1.4697518348693848| 17/17 [00:03<00:00,  4.43it/s]

Epoch 13: 100%|| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.470, emotion_loss=0.171, train_loss=1.210]predict_event weight:  0.5244718                                                                                                                                                                                
predict_event loss:  0.1666383
pleasantness weight:  0.116910346
pleasantness loss:  0.12161065
attention weight:  0.32366374
attention loss:  0.23223639
other_responsblt weight:  0.011398828
other_responsblt loss:  0.1780125
chance_control weight:  0.0148989875
chance_control loss:  0.18495089
social_norms weight:  0.008656534
social_norms loss:  0.15604973
emotion loss:  0.1710751
Epoch 14:  11%| | 17/152 [00:11<01:34,  1.43it/s, v_num=6_40, val_loss=1.470, emotion_loss=0.171, train_losEpoch 14: 100%|| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.470, emotion_loss=0.171, train_loss=1.21007-Jul-24 14:27:21 - INFO - Validation ended. Epoch 14  val_loss: 1.5059905052185059| 17/17 [00:03<00:00,  4.43it/s]

Epoch 14: 100%|| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.510, emotion_loss=0.119, train_loss=1.090]predict_event weight:  0.5254622                                                                                                                                                                                
predict_event loss:  0.15456308
pleasantness weight:  0.115862094
pleasantness loss:  0.12646198
attention weight:  0.32721046
attention loss:  0.20620818
other_responsblt weight:  0.010194821
other_responsblt loss:  0.16799363
chance_control weight:  0.013499625
chance_control loss:  0.17212813
social_norms weight:  0.007770901
social_norms loss:  0.14248693
emotion loss:  0.11865945
Epoch 15: 100%|| 152/152 [01:44<00:00,  1.45it/s, v_num=6_40, val_loss=1.510, emotion_loss=0.119, train_loss=1.09007-Jul-24 14:29:10 - INFO - Validation ended. Epoch 15  val_loss: 1.557945966720581â| 17/17 [00:03<00:00,  4.43it/s]

Epoch 15: 100%|ââââââââââââ| 152/152 [01:49<00:00,  1.39it/s, v_num=6_40, val_loss=1.560, emotion_loss=0.0792, train_loss=1.060]predict_event weight:  0.5212909                                                                                                                                                                                
predict_event loss:  0.16404772
pleasantness weight:  0.11655517
pleasantness loss:  0.121662945
attention weight:  0.33107722
attention loss:  0.20677453
other_responsblt weight:  0.010321879
other_responsblt loss:  0.16433352
chance_control weight:  0.013154568
chance_control loss:  0.17496523
social_norms weight:  0.0076002977
social_norms loss:  0.1445547
emotion loss:  0.079156965
Epoch 16: 100%|ââââââââââââ| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.560, emotion_loss=0.0792, train_loss=1.06007-Jul-24 14:30:59 - INFO - Validation ended. Epoch 16  val_loss: 1.566897988319397â| 17/17 [00:03<00:00,  4.43it/s]

Epoch 16: 100%|| 152/152 [01:48<00:00,  1.40it/s, v_num=6_40, val_loss=1.570, emotion_loss=0.052, train_loss=0.987]predict_event weight:  0.5219371                                                                                                                                                                                
predict_event loss:  0.16068304
pleasantness weight:  0.11716739
pleasantness loss:  0.11769227
attention weight:  0.33183855
attention loss:  0.2064608
other_responsblt weight:  0.0096087605
other_responsblt loss:  0.1512654
chance_control weight:  0.012243641
chance_control loss:  0.15586571
social_norms weight:  0.0072046216
social_norms loss:  0.1425681
emotion loss:  0.052010126
Epoch 17: 100%|| 152/152 [01:44<00:00,  1.46it/s, v_num=6_40, val_loss=1.570, emotion_loss=0.052, train_loss=0.98707-Jul-24 14:32:48 - INFO - Validation ended. Epoch 17  val_loss: 1.5514689683914185| 17/17 [00:03<00:00,  4.43it/s]

Epoch 17: 100%|ââââââââââââ| 152/152 [01:48<00:00,  1.39it/s, v_num=6_40, val_loss=1.550, emotion_loss=0.0339, train_loss=0.953]predict_event weight:  0.52331525                                                                                                                                                                               
predict_event loss:  0.14485703
pleasantness weight:  0.11465247
pleasantness loss:  0.109723456
attention weight:  0.33462986
attention loss:  0.20823559
other_responsblt weight:  0.008965393
other_responsblt loss:  0.14622107
chance_control weight:  0.011825842
chance_control loss:  0.1579851
social_norms weight:  0.0066110776
social_norms loss:  0.15220615
emotion loss:  0.033924688
`Trainer.fit` stopped: `max_epochs=18` reached.
07-Jul-24 14:32:48 - INFO - Training has finished.

Epoch 17: 100%|ââââââââââââ| 152/152 [01:48<00:00,  1.39it/s, v_num=6_40, val_loss=1.550, emotion_loss=0.0339, train_loss=0.953]
07-Jul-24 14:32:48 - INFO - Best validation loss: 1.3863394260406494
[I 2024-07-07 14:32:48,675] Trial 0 finished with value: 1.3863394260406494 and parameters: {'lr': 1e-05, 'train_batch_size': 32}. Best is trial 0 with value: 1.3863394260406494.
07-Jul-24 14:32:48 - INFO - Best trial:
07-Jul-24 14:32:48 - INFO -   Value: 1.39
07-Jul-24 14:32:48 - INFO -   Params: 
07-Jul-24 14:32:48 - INFO -     lr: 1e-05
07-Jul-24 14:32:48 - INFO -     train_batch_size: 32
07-Jul-24 14:32:48 - INFO - Best model saved at: /home1/nekouvag/projects/emo_cog/lightning_logs/2024_07_07__13_56_40/checkpoints/model-epoch-08-val_loss-1.39.ckpt
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home1/nekouvag/projects/emo_cog/myenv/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home1/nekouvag/projects/emo_cog/Main.py ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
07-Jul-24 14:32:50 - INFO - is_distributed:  False
07-Jul-24 14:32:50 - INFO - 

Starting prediction model...
---------------------------------
/home1/nekouvag/projects/emo_cog/myenv/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home1/nekouvag/projects/emo_cog/Main.py ...
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Predicting DataLoader 0: 100%|ââââ| 38/38 [00:08<00:00,  4.47it/s]
07-Jul-24 14:33:11 - INFO - Collected 38 emotion logit batches from predict.
07-Jul-24 14:33:11 - INFO - Collected 38 appriasal label batches from predict.
07-Jul-24 14:33:11 - INFO - emotion logit shape torch.Size([32, 13]) 
07-Jul-24 14:33:11 - INFO - appraisal label shape torch.Size([32, 6]) 
07-Jul-24 14:33:11 - INFO - R2 Score: 0.2089
07-Jul-24 14:33:11 - INFO - R2 Score for predict_event: 0.0673
07-Jul-24 14:33:11 - INFO - R2 Score for pleasantness: 0.6401
07-Jul-24 14:33:11 - INFO - R2 Score for attention: -0.5382
07-Jul-24 14:33:11 - INFO - R2 Score for other_responsblt: 0.3557
07-Jul-24 14:33:11 - INFO - R2 Score for chance_control: 0.2692
07-Jul-24 14:33:11 - INFO - R2 Score for social_norms: 0.4592
07-Jul-24 14:33:11 - INFO - Accuracy: 0.5342
07-Jul-24 14:33:11 - INFO - emo_true_labels_np: (1200,)
07-Jul-24 14:33:11 - INFO - probs:  (1200, 13)
07-Jul-24 14:33:11 - INFO - AUC ROC (One-vs-Rest): 0.8960
07-Jul-24 14:33:11 - INFO - F1 Score: 0.5151